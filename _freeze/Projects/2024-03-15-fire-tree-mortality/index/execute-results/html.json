{
  "hash": "f7785eedff3e8bb17c0bb94ad957ce5e",
  "result": {
    "markdown": "---\ntitle: \"Fire and Tree Mortality \"\ndescription: \"Applying random forest models to predict post-fire tree mortality\"\nauthor: \n - name: Flora Hamilton\n   url: https://floraham.github.io/\n   affiliation: Master of Environmental Data Science Program \n   affiliation-url: https://ucsb-meds.github.io\ndate: 03-15-2024\noutput: html_document\ncategories: [Machine learning, R, MEDS]\ntoc: true\ncitation:\n  url: https://floraham.github.io/Projects/2024-03-15-fire-tree-mortality/\nimage: preview-image.png\n---\n\n\n\n![](preview-image.png)\n\n## Why predict post-fire tree mortality? \nIn the wake of devastating wildfires, one of the most pressing challenges faced by forest managers is assessing the extent of tree mortality. Accurate predictions of post-fire tree survival are crucial for informing management decisions, such as salvage logging operations, reforestation efforts, and ecological restoration strategies. Traditional methods of mortality assessment, which often rely on field surveys and visual inspections, can be time-consuming, labor-intensive, and prone to human error. By leveraging the power of algorithms and datasets, we can develop predictive models that can estimate tree mortality across large burned areas. In this tutorial, I'll explore how to apply machine learning to predict post-wildfire tree mortality. \n\n## The Dataset \nThe database we'll be working with today includes 36,066 observations of individual trees involved in prescribed fires and wildfires occurring over 35 years, from 1981 to 2016. It is a subset of a larger fire and tree mortality database from the US Forest Service (see data description for the full database here: [link](https://www.nature.com/articles/s41597-020-0522-7#Sec10)). Our goal today is to **predict the likelihood of tree mortality after a fire.**\n\n## The Approach \n**Load the libraries we will use for this project**\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(skimr)\nlibrary(tidymodels)\nlibrary(caret)\nlibrary(corrplot)\nlibrary(patchwork)\n```\n:::\n\n\n\n### Data Preparation\n\nOutcome variable: *yr1status* = tree status (0=alive, 1=dead) assessed one year post-fire.\n\nPredictors: *YrFireName, Species, Genus_species, DBH_cm, CVS_percent, BCHM_m, BTL* (Information on these variables available in the database metadata ([link](https://www.fs.usda.gov/rds/archive/products/RDS-2020-0001-2/_metadata_RDS-2020-0001-2.html))).\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#read in file \ntrees_dat<- read_csv(file = \"https://raw.githubusercontent.com/MaRo406/eds-232-machine-learning/main/data/trees-dat.csv\")\n```\n:::\n\n\nRecode all the predictors to a zero_based integer form. \n\n\n::: {.cell}\n\n```{.r .cell-code}\n#recoding the predictors \n\n#create a recipe for the data that defines the target variable \"yr1status\" and all other columns as predictors.\ntrees_recipe <- trees_dat %>%  select(-...1) %>%  \n\n  recipe(yr1status ~ ., data = trees_dat) %>%\n  \n#applies integer encoding to all predictors and starts with 0. this is part of the recipe step.\n  step_integer(all_string(), zero_based = TRUE) %>% \n  \n#preprocessor/preparing to the \"trees_dat\" data. calculates any necessary statistics or parameters based on the provided data, finalizing the recipe.  \nprep(trees_dat) \n  \n#creates a new dataset by applying the preprocessor to the \"trees_recipe or trees_dat\" data.\nbaked_trees <- bake(trees_recipe, new_data = NULL) \n```\n:::\n\n\n### Data Splitting\n\nCreate trees_training (70%) and trees_test (30%) splits for the modeling. This will allow us to train our models on a subset of the data and evaluate their performance on unseen data.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(123) #set seed for reproducibility\ntree_split <- initial_split(data = baked_trees, prop = 0.7) # take the baked dataset and create splits for modeling \ntrees_train <- training(tree_split) #create training data\ntree_test <- testing(tree_split) #create testing data \n```\n:::\n\n\nHow many observations are we using for training with this split?\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Number of observations in the training set\ncat(\"we are using \", nrow(trees_train), \" observations with this split for the training data\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nwe are using  25246  observations with this split for the training data\n```\n:::\n:::\n\n\n### Simple Logistic Regression\n\nLet's start our modeling effort with some simple models: one predictor and one outcome each.\n\nLet's choose, say, three predictors that most highly correlate with our outcome variable for further investigation.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Obtain correlation matrix with our baked dataset \ncorr_mat <- cor(baked_trees %>% select(-...1))\n\n# Make a correlation plot between the variables\ncorrplot(corr_mat, method = \"shade\", shade.col = NA, tl.col = \"black\", tl.srt = 45, addCoef.col = \"black\", cl.pos = \"n\", order = \"original\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-6-1.png){width=672}\n:::\n:::\n\n\nNow we can use glm() to fit three simple logistic regression models, one for each of the predictors we identified.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#fitting three simple logistic regression models, one for each predictor \ncvs_model <- glm(yr1status ~ CVS_percent, family = \"binomial\", data = trees_train)\nbchm_model <- glm(yr1status ~ BCHM_m, family = \"binomial\", data = trees_train)\ndbh_model <- glm(yr1status ~ DBH_cm, family = \"binomial\", data = trees_train)\n```\n:::\n\n\n### Interpret the Coefficients\n\nWe aren't always interested in or able to interpret the model coefficients in a machine learning task. Often predictive accuracy is all we care about.\n\nThat said, let's take a stab at interpreting our model coefficients.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# CVS_percent\nbroom::tidy(cvs_model)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 2 × 5\n  term        estimate std.error statistic p.value\n  <chr>          <dbl>     <dbl>     <dbl>   <dbl>\n1 (Intercept)  -6.61     0.111       -59.4       0\n2 CVS_percent   0.0762   0.00122      62.7       0\n```\n:::\n\n```{.r .cell-code}\n#  BCHM_m \ntidy(bchm_model)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 2 × 5\n  term        estimate std.error statistic p.value\n  <chr>          <dbl>     <dbl>     <dbl>   <dbl>\n1 (Intercept)   -1.86    0.0227      -81.9       0\n2 BCHM_m         0.211   0.00379      55.8       0\n```\n:::\n\n```{.r .cell-code}\n#  DBH_cm \ntidy(dbh_model)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 2 × 5\n  term        estimate std.error statistic  p.value\n  <chr>          <dbl>     <dbl>     <dbl>    <dbl>\n1 (Intercept)   0.546    0.0318       17.2 4.55e-66\n2 DBH_cm       -0.0598   0.00128     -46.7 0       \n```\n:::\n:::\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"A one % increase in pre-fire crown volume scorched (CVS) increases the odds of tree mortality multiplicatively by 1.0791 year after a fire event\"\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"A one meter increase in the maximum burn char (BCHM) increases the probability that a tree will have died multiplicatively by 1.2351 year after a fire event\"\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"A one cm increase in the diameter of the tree at the breast height (DBH) decreases the probability that a tree will have died multiplicatively by 0.0581 year after a fire event\"\n```\n:::\n:::\n\n\n## Visualize \n\nNow let's visualize the results from these models. Plot the fit to the training data of each model.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# visualizing results from the models \n#PLOT OF CVS\ncvs_plot <- ggplot(trees_train, aes(x=CVS_percent, y=yr1status)) + geom_point(alpha = 0.2) +\n      stat_smooth(method=\"glm\",  se=TRUE,\n                method.args = list(family=binomial)) +\n  theme_bw() +\n  labs(x = \"Crown Volume Scorched %\",\n       y = \"Year 1 Status (0 = alive, 1 = dead)\",\n       title = \"Logistic regression of CVS\")\n\n\n#PLOT OF MAXIMUM BURN CHAR AREA\nbchm_plot <- ggplot(trees_train, aes(x=BCHM_m, y=yr1status)) + geom_point(alpha = 0.2) +\n      stat_smooth(method=\"glm\",  se=TRUE,\n                method.args = list(family=binomial)) +\n  theme_bw() +\n  labs(x = \"Maximum Burn Char (m)\",\n       y = \"Year 1 Status (0 = alive, 1 = dead)\",\n       title = \"Logistic regression of BCHM\")\n \n \n #PLOT OF DIAMETER AT BREAST HEIGHT\ndbh_plot <- ggplot(trees_train, aes(x=DBH_cm, y=yr1status)) + geom_point(alpha = 0.2) +\n      stat_smooth(method=\"glm\",  se=TRUE,\n                method.args = list(family=binomial)) +\n  theme_bw() +\n  labs(x = \"Diameter at breast height (cm)\",\n       y = \"Year 1 Status (0 = alive, 1 = dead)\",\n       title = \"Logistic regression of DBH\")\n\n### arrange plots together \n\ncvs_plot + bchm_plot + dbh_plot+ plot_layout(ncol=2)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-10-1.png){width=672}\n:::\n:::\n\n\n### Multiple Logistic Regression\n\nLet's not limit ourselves to a single-predictor model. More predictors might lead to better model performance.\n\nUse glm() to fit a multiple logistic regression called \"logistic_full\", with all three of the predictors included. Which of these are significant in the resulting model?\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Fit a multiple logistic regression model\nlogistic_full <- glm(yr1status ~ CVS_percent + BCHM_m + DBH_cm, family = \"binomial\", data = trees_train)\n\n# Display the summary of the model\nsummary(logistic_full)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nglm(formula = yr1status ~ CVS_percent + BCHM_m + DBH_cm, family = \"binomial\", \n    data = trees_train)\n\nCoefficients:\n             Estimate Std. Error z value Pr(>|z|)    \n(Intercept) -4.762446   0.115367  -41.28   <2e-16 ***\nCVS_percent  0.061454   0.001177   52.20   <2e-16 ***\nBCHM_m       0.159395   0.005438   29.31   <2e-16 ***\nDBH_cm      -0.060724   0.001945  -31.21   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 30086  on 25245  degrees of freedom\nResidual deviance: 13282  on 25242  degrees of freedom\nAIC: 13290\n\nNumber of Fisher Scoring iterations: 7\n```\n:::\n:::\n\n\nFor the logistic model with three predictors (CVS, BCHM, and DBH), all the predictors are significant in the model as their p-values are near zero.  \n\n\n### Estimate Model Accuracy\n\nNow we want to estimate our model's generalizability using resampling.\n\nLet's use cross validation to assess model accuracy. We can use caret::train() to fit four 10-fold cross-validated models (cv_model1, cv_model2, cv_model3, cv_model4) that correspond to each of the four models we've fit so far: three simple logistic regression models corresponding to each of the three key predictors (CVS_percent, DBH_cm, BCHM_m) and a multiple logistic regression model that combines all three predictors.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#convert yr1status to a factor\ntrees_train <- trees_train %>%\n  mutate(yr1status = as.factor(yr1status)) # converted the outcome variable to a factor\n\n#CVS model\ncv_model1 <- train(\n  yr1status ~ CVS_percent, \n  data = trees_train, \n  method = \"glm\",\n  family = \"binomial\",\n  trControl = trainControl(method = \"cv\", number = 10)\n)\n\n#BCHM model\ncv_model2 <- train(\n  yr1status ~ BCHM_m, \n  data = trees_train, \n  method = \"glm\",\n  family = \"binomial\",\n  trControl = trainControl(method = \"cv\", number = 10))\n\n#DBH model\ncv_model3 <- train(\n  yr1status ~ DBH_cm, \n  data = trees_train, \n  method = \"glm\",\n  family = \"binomial\",\n  trControl = trainControl(method = \"cv\", number = 10))\n\n#Full Model\ncv_model4 <- train(\n  yr1status ~ CVS_percent + BCHM_m + DBH_cm, \n  data = trees_train, \n  method = \"glm\",\n  family = \"binomial\",\n  trControl = trainControl(method = \"cv\", number = 10))\n```\n:::\n\n\n#### Use caret::resamples() to extract then compare the classification accuracy for each model. However, resamples() wont give you what you need unless you convert the outcome variable to factor form. Which model has the highest accuracy?\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#comparing the classification accuracy of the models \nsummary(\n  resamples(\n    list(\n      model1 = cv_model1, \n      model2 = cv_model2, \n      model3 = cv_model3,\n      model4 = cv_model4\n    )\n  )\n)$statistics$Accuracy\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n            Min.   1st Qu.    Median      Mean   3rd Qu.      Max. NA's\nmodel1 0.8887129 0.8918708 0.8976238 0.8975291 0.9004556 0.9152139    0\nmodel2 0.7631683 0.7662441 0.7718365 0.7718450 0.7770627 0.7817822    0\nmodel3 0.7473267 0.7482178 0.7496533 0.7508912 0.7520545 0.7596040    0\nmodel4 0.8918812 0.9009901 0.9057052 0.9041038 0.9086139 0.9104950    0\n```\n:::\n:::\n\n\n**Model 4 has seems to have the highest mean accuracy (0.904) compared to the other models. The other models CVS, DBH, and BCHM have mean accuracy values between 0.75-0.89. **\n\nLet's move forward with this single most accurate model.\n\n#### Compute the confusion matrix and overall fraction of correct predictions by the model.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#making predictions on the training data \npred_trees <- predict(cv_model4, data = tree_train)\n\n#confusion matrix using the model predictions \ncm <- confusionMatrix(\n  data = factor(pred_trees),\n  reference = factor(trees_train$yr1status)\n)\n\ncm_TN00 <- cm$table[1] #True Negative\ncm_FP10 <- cm$table[3] #False Negative\ncm_FN01 <- cm$table[2] #False Positive\ncm_TP11 <- cm$table[4] #True Positive \n```\n:::\n\n\nThe overall fraction is (True Positives + True Negatives) / (Total Count of Predictions)\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n```\nThe overall fraction of correct predictions computed is 0.904 \nThe accuracy of the model is calculated by the number of true positive ( 6295 ) and true negative predictions ( 16527 ) over all of the predictions made\n```\n:::\n:::\n\n\n#### What confusion matrix is telling us about the types of mistakes made by logistic regression.\n\n**The confusion matrix gives the number of true negatives, false negatives (Type 2 error), false positives (Type 1 error) and true positives made in the logistic regression. We see that there are 852 false negatives (Type 2 error) and 1572 false positives (Type 1 error) made by the logistic regression. This means that there are twice as many false positives as there are false negatives. In this case, it is saying that the logistic regression is twice as often to predict that trees will burn when they won't burn, than that trees won't burn when they will burn.**\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncm$table\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n          Reference\nPrediction     0     1\n         0 16527   852\n         1  1572  6295\n```\n:::\n:::\n\n\n**You can use this information to show the accuracy (described in Q11), the specificity (the number of correct negative predictions divided by the total number of actual negatives), and the sensitivity (the number of correct positive predictions over the total number of positives).**\n\n> What is the overall accuracy of the model? \n\n**The accuracy of the model is .904 and is calculated by the total number of correct predictions divided by the total number of predictions (correct and incorrect predictions). It tells us that the model is 90.4% accurate.**\n\n### Test Final Model\n\nAlright, now we'll take our most accurate model and make predictions on some unseen data (the test data).\n\n#### Now that we have identified our best model, evaluate it by running a prediction on the test data, trees_test.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#making the yr1ststus a factor \ntree_test <- tree_test %>%\n  mutate(yr1status = as.factor(yr1status)) \n\n#using the model to make predictions on the test data\npred_test_trees <- predict(cv_model4, newdata = tree_test)\n\n#confusion matrix for the test data predictions\ncv4_cm <- confusionMatrix(\n  data = factor(pred_test_trees),\n  reference = factor(tree_test$yr1status)\n)\ncv4_cm\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nConfusion Matrix and Statistics\n\n          Reference\nPrediction    0    1\n         0 7032  366\n         1  702 2720\n                                          \n               Accuracy : 0.9013          \n                 95% CI : (0.8955, 0.9069)\n    No Information Rate : 0.7148          \n    P-Value [Acc > NIR] : < 2.2e-16       \n                                          \n                  Kappa : 0.7656          \n                                          \n Mcnemar's Test P-Value : < 2.2e-16       \n                                          \n            Sensitivity : 0.9092          \n            Specificity : 0.8814          \n         Pos Pred Value : 0.9505          \n         Neg Pred Value : 0.7949          \n             Prevalence : 0.7148          \n         Detection Rate : 0.6499          \n   Detection Prevalence : 0.6837          \n      Balanced Accuracy : 0.8953          \n                                          \n       'Positive' Class : 0               \n                                          \n```\n:::\n:::\n\n\n> How does the accuracy of this final model on the test data compare to its cross validation accuracy? \n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"Accuracy of final mode: 0.901293900184843\"\n```\n:::\n:::\n\n\nCV accuracy results\n\n```         \n            Min.   1st Qu.    Median      Mean   3rd Qu.      Max. NA's\nmodel4 0.8958416 0.8975248 0.9031491 0.9024406 0.9060833 0.9092710    0\n```\n\n**The accuracy of this final model is very similar to that of the cross validation accuracy on the test data. This is not surprising as the variables included in the model were pretty well correlated with tree death. Also because there is a class imbalance in the response variable (given by the \"No information rate\", or proportion of observations that fall within the majority class, which indicates that \\~71% of the trees are living); so, if the model just used that statistic for prediction it would be right roughly 71% of the time. Using other variables to improve the model to by an additional 20% seems reasonable.**\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}